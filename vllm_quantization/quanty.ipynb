{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70743914-ed51-44d6-bac7-2be4377ac69d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npython3 -m pip install vllm\\npython3 -m pip install boto3\\n\\ncurl https://dl.min.io/client/mc/release/linux-amd64/mc   --create-dirs   -o $HOME/minio-binaries/mc\\nchmod +x $HOME/minio-binaries/mc\\n/opt/app-root/src/minio-binaries/mc --help\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "python3 -m pip install vllm\n",
    "python3 -m pip install boto3\n",
    "\n",
    "#curl https://dl.min.io/client/mc/release/linux-amd64/mc \\\n",
    "#  --create-dirs \\\n",
    "#  -o $HOME/minio-binaries/mc\n",
    "#chmod +x $HOME/minio-binaries/mc\n",
    "#/opt/app-root/src/minio-binaries/mc --help\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20de58ff-558c-424a-bf28-714428b4fc54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"s3_host\"] = \"http://minio-api-http-model-serving-test.apps.psap.example.com\"\n",
    "os.environ[\"s3_access_key\"] = \"minio\"\n",
    "os.environ[\"s3_secret_access_key\"] = \"minio_1_2_3\"\n",
    "os.environ[\"s3_bucket\"] = \"models\"\n",
    "os.environ[\"model_name\"] = \"granite-3b-code-instruct/granite-3b-code-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3ebfed2d-fc2d-49da-8a3e-25c30b4065d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_model_from_s3(model_name, destination_path):\n",
    "    # Create S3 client\n",
    "    s3_client = client(\n",
    "        's3', endpoint_url=s3_endpoint_url, aws_access_key_id=s3_access_key,\n",
    "        aws_secret_access_key=s3_secret_key, verify=False\n",
    "    )\n",
    "\n",
    "    # List all objects in the folder\n",
    "    objects = s3_client.list_objects(Bucket=s3_bucket_name, Prefix=model_name)\n",
    "\n",
    "    # Download each object in the folder\n",
    "    for obj in objects.get('Contents', []):\n",
    "        file_name = obj['Key']\n",
    "        local_file_name = os.path.join(destination_path, file_name.replace(model_name, '')[1:])\n",
    "        if not os.path.exists(os.path.dirname(local_file_name)):\n",
    "            try:\n",
    "                os.makedirs(os.path.dirname(local_file_name))\n",
    "            except OSError as exc:  # Guard against race condition\n",
    "                if exc.errno != errno.EEXIST:\n",
    "                    print(\"Error downloading model\")\n",
    "                    raise\n",
    "        s3_client.download_file(s3_bucket_name, file_name, local_file_name)\n",
    "\n",
    "    print('Model downloaded successfully from S3.')\n",
    "\n",
    "def upload_model_to_s3(source_path, destination_prefix):\n",
    "    s3_client = client(\n",
    "        's3', endpoint_url=s3_endpoint_url, aws_access_key_id=s3_access_key,\n",
    "        aws_secret_access_key=s3_secret_key, verify=False\n",
    "    )\n",
    "    print(source_path)\n",
    "    print(os.walk(source_path))\n",
    "    for root, dirs, files in os.walk(source_path):\n",
    "        for file in files:\n",
    "            print(f\"Uploading: '{file}'\")\n",
    "            file_path = os.path.join(root, file)\n",
    "            s3_client.upload_file(file_path, s3_bucket_name, f\"{destination_prefix}/{file}\")\n",
    "\n",
    "    print(f\"Quantized model uploaded to MinIO bucket '{s3_bucket_name}' as '{destination_prefix}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9b0cb90-44b9-4512-8dbc-e39f90b86f91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded successfully from S3.\n"
     ]
    }
   ],
   "source": [
    "model=os.environ[\"model_name\"]\n",
    "path=os.environ[\"s3_bucket\"]+'/'+os.environ[\"model_name\"]\n",
    "download_model_from_s3(model, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d24bc4c5-0a82-4d7c-881a-b1f1d849cfc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quantize_gpu_model(model_path:str, compress_model_path: str, ds: str):\n",
    "    # Quantizing an LLM\n",
    "    from transformers import AutoTokenizer\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "\n",
    "    MAX_SEQ_LEN = 512\n",
    "    NUM_EXAMPLES = 512\n",
    "\n",
    "    def preprocess(example):\n",
    "        return {\"text\": tokenizer.apply_chat_template(example[\"messages\"],\n",
    "                                                      tokenize=False)}\n",
    "\n",
    "    print(\"Loading the dataset and tokenizers\")\n",
    "    dataset = load_dataset(ds, split=\"train_sft\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    ds = dataset.shuffle().select(range(NUM_EXAMPLES))\n",
    "    ds = ds.map(preprocess)\n",
    "\n",
    "    examples = [\n",
    "        tokenizer(\n",
    "            example[\"text\"], padding=False, max_length=MAX_SEQ_LEN,\n",
    "            truncation=True,\n",
    "        ) for example in ds\n",
    "    ]\n",
    "\n",
    "    print(\"Loaded the dataset and tokenizers\")\n",
    "    print(\"Starting the quantization\")\n",
    "\n",
    "    # Apply GPTQ\n",
    "    quantize_config = BaseQuantizeConfig(\n",
    "        bits=8,                         # Only support 4 bit\n",
    "        group_size=-1,                 # Set to g=128 or -1 (for channelwise)\n",
    "        desc_act=False,                 # Marlin does not support act_order=True\n",
    "        model_file_base_name=\"model\",   # Name of the model.safetensors when we call save_pretrained\n",
    "    )\n",
    "    print(\"Applying GPTQ for quantization\")\n",
    "\n",
    "    model = AutoGPTQForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        quantize_config,\n",
    "        device_map=\"auto\")\n",
    "    model.quantize(examples)\n",
    "\n",
    "    gptq_save_dir = f\"{model_path}-gptq\"\n",
    "    print(f\"Saving gptq model to {gptq_save_dir}\")\n",
    "    model.save_pretrained(gptq_save_dir)\n",
    "    tokenizer.save_pretrained(gptq_save_dir)\n",
    "\n",
    "    # Convert to Marlin\n",
    "    #print(\"Reloading in marlin format\")\n",
    "    #marlin_model = AutoGPTQForCausalLM.from_quantized(\n",
    "    #    gptq_save_dir,\n",
    "    #    use_marlin=True,\n",
    "    #    device_map=\"auto\")\n",
    "\n",
    "    print(f\"Saving model in marlin format to {compress_model_path}\")\n",
    "    #marlin_model.save_pretrained(compress_model_path)\n",
    "    #tokenizer.save_pretrained(compress_model_path)\n",
    "\n",
    "    print(\"Quantization process completed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8cf03eb8-1e13-4c62-a167-5d6b5252fff4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset and tokenizers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24fc77f79cac442496bb3d7c17e80524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/512 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the dataset and tokenizers\n",
      "Starting the quantization\n",
      "Applying GPTQ for quantization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1bff789f80f418a9c417a0140d26c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Start quantizing layer 1/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 1/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 1/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 1/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 1/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 1/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 1/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 1/32...\n",
      "INFO - Start quantizing layer 2/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 2/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 2/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 2/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 2/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 2/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 2/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 2/32...\n",
      "INFO - Start quantizing layer 3/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 3/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 3/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 3/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 3/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 3/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 3/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 3/32...\n",
      "INFO - Start quantizing layer 4/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 4/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 4/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 4/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 4/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 4/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 4/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 4/32...\n",
      "INFO - Start quantizing layer 5/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 5/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 5/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 5/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 5/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 5/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 5/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 5/32...\n",
      "INFO - Start quantizing layer 6/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 6/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 6/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 6/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 6/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 6/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 6/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 6/32...\n",
      "INFO - Start quantizing layer 7/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 7/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 7/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 7/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 7/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 7/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 7/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 7/32...\n",
      "INFO - Start quantizing layer 8/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 8/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 8/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 8/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 8/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 8/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 8/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 8/32...\n",
      "INFO - Start quantizing layer 9/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 9/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 9/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 9/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 9/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 9/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 9/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 9/32...\n",
      "INFO - Start quantizing layer 10/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 10/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 10/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 10/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 10/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 10/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 10/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 10/32...\n",
      "INFO - Start quantizing layer 11/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 11/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 11/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 11/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 11/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 11/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 11/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 11/32...\n",
      "INFO - Start quantizing layer 12/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 12/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 12/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 12/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 12/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 12/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 12/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 12/32...\n",
      "INFO - Start quantizing layer 13/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 13/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 13/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 13/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 13/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 13/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 13/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 13/32...\n",
      "INFO - Start quantizing layer 14/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 14/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 14/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 14/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 14/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 14/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 14/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 14/32...\n",
      "INFO - Start quantizing layer 15/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 15/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 15/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 15/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 15/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 15/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 15/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 15/32...\n",
      "INFO - Start quantizing layer 16/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 16/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 16/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 16/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 16/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 16/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 16/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 16/32...\n",
      "INFO - Start quantizing layer 17/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 17/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 17/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 17/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 17/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 17/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 17/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 17/32...\n",
      "INFO - Start quantizing layer 18/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 18/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 18/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 18/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 18/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 18/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 18/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 18/32...\n",
      "INFO - Start quantizing layer 19/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 19/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 19/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 19/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 19/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 19/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 19/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 19/32...\n",
      "INFO - Start quantizing layer 20/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 20/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 20/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 20/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 20/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 20/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 20/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 20/32...\n",
      "INFO - Start quantizing layer 21/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 21/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 21/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 21/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 21/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 21/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 21/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 21/32...\n",
      "INFO - Start quantizing layer 22/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 22/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 22/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 22/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 22/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 22/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 22/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 22/32...\n",
      "INFO - Start quantizing layer 23/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 23/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 23/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 23/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 23/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 23/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 23/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 23/32...\n",
      "INFO - Start quantizing layer 24/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 24/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 24/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 24/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 24/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 24/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 24/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 24/32...\n",
      "INFO - Start quantizing layer 25/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 25/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 25/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 25/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 25/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 25/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 25/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 25/32...\n",
      "INFO - Start quantizing layer 26/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 26/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 26/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 26/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 26/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 26/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 26/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 26/32...\n",
      "INFO - Start quantizing layer 27/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 27/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 27/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 27/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 27/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 27/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 27/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 27/32...\n",
      "INFO - Start quantizing layer 28/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 28/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 28/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 28/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 28/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 28/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 28/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 28/32...\n",
      "INFO - Start quantizing layer 29/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 29/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 29/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 29/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 29/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 29/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 29/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 29/32...\n",
      "INFO - Start quantizing layer 30/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 30/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 30/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 30/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 30/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 30/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 30/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 30/32...\n",
      "INFO - Start quantizing layer 31/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 31/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 31/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 31/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 31/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 31/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 31/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 31/32...\n",
      "INFO - Start quantizing layer 32/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 32/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 32/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 32/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 32/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 32/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 32/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 32/32...\n",
      "WARNING - you are using save_pretrained, which will re-direct to save_quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving gptq model to models/granite-3b-code-instruct/granite-3b-code-instruct-gptq\n",
      "Saving model in marlin format to models/granite-3b-code-instruct/granite-3b-code-instruct/compressed\n",
      "Quantization process completed\n"
     ]
    }
   ],
   "source": [
    "model_path=os.environ[\"s3_bucket\"]+'/'+os.environ[\"model_name\"]\n",
    "\n",
    "compress_model_path = model_path+\"/compressed\"\n",
    "dataset_name = \"HuggingFaceH4/ultrachat_200k\"  # Replace with the name of your dataset\n",
    "\n",
    "quantize_gpu_model(model_path, compress_model_path, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bb7ae312-e394-4d0c-a84a-3f6e4961bbc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/granite-3b-code-instruct/granite-3b-code-instruct-gptq\n",
      "<generator object _walk at 0x7f8a8b1712e0>\n",
      "Uploading: 'model.safetensors'\n",
      "Uploading: 'config.json'\n",
      "Uploading: 'quantize_config.json'\n",
      "Uploading: 'tokenizer_config.json'\n",
      "Uploading: 'special_tokens_map.json'\n",
      "Uploading: 'vocab.json'\n",
      "Uploading: 'merges.txt'\n",
      "Uploading: 'tokenizer.json'\n",
      "Quantized model uploaded to MinIO bucket 'models' as 'granite-3b-code-instruct/granite-3b-code-instruct-gptq'.\n"
     ]
    }
   ],
   "source": [
    "model=os.environ[\"model_name\"]\n",
    "bucket=os.environ[\"s3_bucket\"]\n",
    "path = f\"{bucket}/{model}-gptq\"\n",
    "prefix = f\"{model}-gptq\"\n",
    "upload_model_to_s3(path, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5621146d-fff4-4d03-8c1f-8415da0817b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70743914-ed51-44d6-bac7-2be4377ac69d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "python3 -m pip install vllm\n",
    "python3 -m pip install boto3\n",
    "\n",
    "#curl https://dl.min.io/client/mc/release/linux-amd64/mc \\\n",
    "#  --create-dirs \\\n",
    "#  -o $HOME/minio-binaries/mc\n",
    "#chmod +x $HOME/minio-binaries/mc\n",
    "#/opt/app-root/src/minio-binaries/mc --help\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20de58ff-558c-424a-bf28-714428b4fc54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import errno\n",
    "from boto3 import client\n",
    "\n",
    "os.environ[\"s3_host\"] = \"http://minio-api-http-model-serving-test.apps.psap.example.com\"\n",
    "os.environ[\"s3_access_key\"] = \"minio\"\n",
    "os.environ[\"s3_secret_access_key\"] = \"minio_1_2_3\"\n",
    "os.environ[\"s3_bucket\"] = \"models\"\n",
    "os.environ[\"model_name\"] = \"granite-3b-code-instruct/granite-3b-code-instruct\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebfed2d-fc2d-49da-8a3e-25c30b4065d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_model_from_s3(model_name, destination_path):\n",
    "    # Create S3 client\n",
    "    s3_client = client(\n",
    "        's3', endpoint_url=os.environ[\"s3_host\"], aws_access_key_id=os.environ[\"s3_access_key\"],\n",
    "        aws_secret_access_key=os.environ[\"s3_secret_access_key\"], verify=False\n",
    "    )\n",
    "\n",
    "    # List all objects in the folder\n",
    "    objects = s3_client.list_objects(Bucket=os.environ[\"s3_bucket\"], Prefix=os.environ[\"model_name\"])\n",
    "\n",
    "    # Download each object in the folder\n",
    "    for obj in objects.get('Contents', []):\n",
    "        file_name = obj['Key']\n",
    "        local_file_name = os.path.join(destination_path, file_name.replace(model_name, '')[1:])\n",
    "        if not os.path.exists(os.path.dirname(local_file_name)):\n",
    "            try:\n",
    "                os.makedirs(os.path.dirname(local_file_name))\n",
    "            except OSError as exc:  # Guard against race condition\n",
    "                if exc.errno != errno.EEXIST:\n",
    "                    print(\"Error downloading model\")\n",
    "                    raise\n",
    "        s3_client.download_file(os.environ[\"s3_bucket\"], file_name, local_file_name)\n",
    "\n",
    "    print('Model downloaded successfully from S3.')\n",
    "\n",
    "def upload_model_to_s3(source_path, destination_prefix):\n",
    "    s3_client = client(\n",
    "        's3', endpoint_url=os.environ[\"s3_host\"], aws_access_key_id=os.environ[\"s3_access_key\"],\n",
    "        aws_secret_access_key=os.environ[\"s3_secret_access_key\"], verify=False\n",
    "    )\n",
    "    print(source_path)\n",
    "    print(os.walk(source_path))\n",
    "    for root, dirs, files in os.walk(source_path):\n",
    "        for file in files:\n",
    "            print(f\"Uploading: '{file}'\")\n",
    "            file_path = os.path.join(root, file)\n",
    "            s3_client.upload_file(file_path, os.environ[\"s3_bucket\"], f\"{destination_prefix}/{file}\")\n",
    "\n",
    "    print(f\"Quantized model uploaded to MinIO bucket as '{destination_prefix}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b0cb90-44b9-4512-8dbc-e39f90b86f91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model=os.environ[\"model_name\"]\n",
    "path=os.environ[\"s3_bucket\"]+'/'+os.environ[\"model_name\"]\n",
    "download_model_from_s3(model, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24bc4c5-0a82-4d7c-881a-b1f1d849cfc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quantize_gpu_model(model_path:str, compress_model_path: str, ds: str):\n",
    "    # Quantizing an LLM\n",
    "    from transformers import AutoTokenizer\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "\n",
    "    MAX_SEQ_LEN = 512\n",
    "    NUM_EXAMPLES = 512\n",
    "\n",
    "    def preprocess(example):\n",
    "        return {\"text\": tokenizer.apply_chat_template(example[\"messages\"],\n",
    "                                                      tokenize=False)}\n",
    "\n",
    "    print(\"Loading the dataset and tokenizers\")\n",
    "    dataset = load_dataset(ds, split=\"train_sft\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    ds = dataset.shuffle().select(range(NUM_EXAMPLES))\n",
    "    ds = ds.map(preprocess)\n",
    "\n",
    "    examples = [\n",
    "        tokenizer(\n",
    "            example[\"text\"], padding=False, max_length=MAX_SEQ_LEN,\n",
    "            truncation=True,\n",
    "        ) for example in ds\n",
    "    ]\n",
    "\n",
    "    print(\"Loaded the dataset and tokenizers\")\n",
    "    print(\"Starting the quantization\")\n",
    "\n",
    "    # Apply GPTQ\n",
    "    quantize_config = BaseQuantizeConfig(\n",
    "        bits=8,                         # Only support 4 bit\n",
    "        group_size=-1,                 # Set to g=128 or -1 (for channelwise)\n",
    "        desc_act=False,                 # Marlin does not support act_order=True\n",
    "        model_file_base_name=\"model\",   # Name of the model.safetensors when we call save_pretrained\n",
    "    )\n",
    "    print(\"Applying GPTQ for quantization\")\n",
    "\n",
    "    model = AutoGPTQForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        quantize_config,\n",
    "        device_map=\"auto\")\n",
    "    model.quantize(examples)\n",
    "\n",
    "    gptq_save_dir = f\"{model_path}-gptq\"\n",
    "    print(f\"Saving gptq model to {gptq_save_dir}\")\n",
    "    model.save_pretrained(gptq_save_dir)\n",
    "    tokenizer.save_pretrained(gptq_save_dir)\n",
    "\n",
    "    # Convert to Marlin\n",
    "    #print(\"Reloading in marlin format\")\n",
    "    #marlin_model = AutoGPTQForCausalLM.from_quantized(\n",
    "    #    gptq_save_dir,\n",
    "    #    use_marlin=True,\n",
    "    #    device_map=\"auto\")\n",
    "\n",
    "    #print(f\"Saving model in marlin format to {compress_model_path}\")\n",
    "    #marlin_model.save_pretrained(compress_model_path)\n",
    "    #tokenizer.save_pretrained(compress_model_path)\n",
    "\n",
    "    print(\"Quantization process completed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf03eb8-1e13-4c62-a167-5d6b5252fff4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path=os.environ[\"s3_bucket\"]+'/'+os.environ[\"model_name\"]\n",
    "\n",
    "compress_model_path = model_path+\"/compressed\"\n",
    "dataset_name = \"HuggingFaceH4/ultrachat_200k\"  # Replace with the name of your dataset\n",
    "\n",
    "quantize_gpu_model(model_path, compress_model_path, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7ae312-e394-4d0c-a84a-3f6e4961bbc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model=os.environ[\"model_name\"]\n",
    "bucket=os.environ[\"s3_bucket\"]\n",
    "path = f\"{bucket}/{model}-gptq\"\n",
    "prefix = f\"{model}-gptq\"\n",
    "upload_model_to_s3(path, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5621146d-fff4-4d03-8c1f-8415da0817b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
